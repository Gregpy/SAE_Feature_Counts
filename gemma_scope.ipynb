{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code used from (run without cuda)\n",
    "\n",
    "https://colab.research.google.com/drive/17dQFYUYnuKnP6OwQPH9v_GSYUW5aj-Rp\n",
    "\n",
    "https://huggingface.co/google/gemma-scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    device_map='auto',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,  18925,    692,    614,   3326,    577,   5056,   1593,   1069,\n",
      "           2177,    476,  47420,  18216, 235336]])\n",
      "<bos>Would you be able to travel through time using a wormhole?\n",
      "\n",
      "[Answer 1]\n",
      "\n",
      "Yes, you can travel through time using a wormhole.\n",
      "\n",
      "A wormhole is a theoretical object that connects two points in space-time. It is a tunnel through space-time that allows objects to travel from\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Would you be able to travel through time using a wormhole?\"\n",
    "\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True)#.to(\"cuda\")\n",
    "print(inputs)\n",
    "\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n",
    "    force_download=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = np.load(path_to_params)\n",
    "# pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}\n",
    "# Load SAE params without forcing .cuda()\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v) for k, v in params.items()}  # ← no .cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_dec': torch.Size([16384, 2304]),\n",
       " 'W_enc': torch.Size([2304, 16384]),\n",
       " 'b_dec': torch.Size([2304]),\n",
       " 'b_enc': torch.Size([16384]),\n",
       " 'threshold': torch.Size([16384])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v.shape for k, v in pt_params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2101, 1.1695, 0.9836,  ..., 1.0630, 0.9997, 1.1070])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_params[\"W_enc\"].norm(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class JumpReLUSAE(nn.Module):\n",
    "  def __init__(self, d_model, d_sae):\n",
    "    # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
    "    # If you want to train your own SAEs then we recommend using blah\n",
    "    super().__init__()\n",
    "    self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
    "    self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
    "    self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "  def encode(self, input_acts):\n",
    "    pre_acts = input_acts @ self.W_enc + self.b_enc\n",
    "    mask = (pre_acts > self.threshold)\n",
    "    acts = mask * torch.nn.functional.relu(pre_acts)\n",
    "    return acts\n",
    "\n",
    "  def decode(self, acts):\n",
    "    return acts @ self.W_dec + self.b_dec\n",
    "\n",
    "  def forward(self, acts):\n",
    "    acts = self.encode(acts)\n",
    "    recon = self.decode(acts)\n",
    "    return recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae.load_state_dict(pt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# sae = sae.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gather_residual_activations(model, target_layer, inputs):\n",
    "  target_act = None\n",
    "  def gather_target_act_hook(mod, inputs, outputs):\n",
    "    nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
    "    target_act = outputs[0]\n",
    "    return outputs\n",
    "  handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
    "  _ = model.forward(inputs)\n",
    "  handle.remove()\n",
    "  return target_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_act = gather_residual_activations(model, 20, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts = sae.encode(target_act.to(torch.float32))\n",
    "recon = sae.decode(sae_acts)\n",
    "\n",
    "\n",
    "# device = next(sae.parameters()).device  # Automatically get SAE's device\n",
    "# target_act = target_act.to(device=device, dtype=torch.float32)\n",
    "\n",
    "# sae_acts = sae.encode(target_act)\n",
    "# recon = sae.decode(sae_acts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8887)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) **2) / (target_act[:, 1:].to(torch.float32).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7017,   47,   65,   70,   55,   72,   65,   75,   80,   72,   68,   93,\n",
       "           86,   89]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sae_acts > 1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6631,  5482, 10376,  1670, 11023,  7562,  9407,  8399, 12935, 10004,\n",
       "         10004, 10004, 12935,  3442]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, inds = sae_acts.max(-1)\n",
    "\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_1 = \"inuit\"\n",
    "keyword_2 = \"americans\"\n",
    "\n",
    "template = \"The {} have a long history and culture.\"\n",
    "prompt_1 = template.format(keyword_1)\n",
    "prompt_2 = template.format(keyword_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = tokenizer(prompt_1, return_tensors=\"pt\")  # .to(\"cuda\")\n",
    "inputs_2 = tokenizer(prompt_2, return_tensors=\"pt\")  # .to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_1 = gather_residual_activations(model, 20, inputs_1[\"input_ids\"])\n",
    "act_2 = gather_residual_activations(model, 20, inputs_2[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inuit → 698 active features\n",
      "americans → 685 active features\n",
      "Difference → 13\n"
     ]
    }
   ],
   "source": [
    "sae_acts_1 = sae.encode(act_1.to(torch.float32))\n",
    "sae_acts_2 = sae.encode(act_2.to(torch.float32))\n",
    "\n",
    "# count how many features fired per token\n",
    "active_feats_1 = (sae_acts_1[:, 1:] > 0).sum().item()\n",
    "active_feats_2 = (sae_acts_2[:, 1:] > 0).sum().item()\n",
    "\n",
    "print(f\"{keyword_1} → {active_feats_1} active features\")\n",
    "print(f\"{keyword_2} → {active_feats_2} active features\")\n",
    "print(f\"Difference → {abs(active_feats_1 - active_feats_2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
